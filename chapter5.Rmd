# Dimensionality reduction techniques # 

*This week I have covered  Dimensionality reduction techniques and data wrangling exercise. *

```{r}
date()
```
### 0. Information of prepared Human dataset

```{r}

# read the human data from URL
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", sep  =",", header = T)
# human <- read.table("data/human.csv", sep = ";")

dim(human)

# look at the (column) names of human
names(human)

```

The origin of the data is  the United Nations Development Programme. This dataset contains information of 155 countries. This dataset has 8 columns. The data attributes include features such as Gross National Income, Life expectancy at birth, Expected years of schooling  and so on. 

You may see all features explained in brief.

- Edu2.FM : Proportion of females with at least secondary education / Proportion of males with at least secondary education
- Labo.FM  : Proportion of females in the labour force / Proportion of males in the labour force
- Edu.Exp : Expected years of schooling 
- Life.Exp : Life expectancy at birth
- GNI : Gross National Income 
- Mat.Mor : Maternal mortality ratio
- Ado.Birth : Adolescent birth rate
- Parli.F : Percetange of female representatives in parliament


```{r}


# look at the structure of human
str(human)

```

```{r}


# print out summaries of the variables
summary(human)

```



### 1.1  Graphical overview of the data


```{r}

# Access GGally
library(GGally)
library(corrplot)

# visualize the 'human' variables
ggpairs(human)

# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot

 
corrplot.mixed(cor(human, use = "pairwise"), order ="hclust")
```


### 1.2 Interpretation of results

We can Immediately notice that,

 
 - **Mat.Mor**  has a strong negative correlation with **Life.Exp**. This is due to the fact that when Maternal mortality ratio increases Life expectancy at birth decreases.
 - We do not see a strong correlation between  **GNI** variable  and **Labo.FM**. That is because Gross National Income does not depend on Proportion of females in the labour force / Proportion of males in the labour force variable. These two variables can be considered as almost independent.
  - **Life.Exp** has a strong positive correlation to **Edu.Exp**. It is obvious that Expected years of schooling increases when Life expectancy at birth increases.
  - **Life.Exp** has a strong correlation to  both **Mat.Mor** and  **Ado.Birth**.  That is due to the fact that Life expectancy at births has a negative relationship with both  Maternal mortality ratio and Adolescent birth rate.
 - **Ado.Birth** has a positive correlation with **Mat.Mor**. That is because Maternal mortality ratio increases with Adolescent birth rate.
 - We do not see a strong correlation between  **Edu.Exp** variable  and **Labo.FM**. That is because Expected years of schooling does not depend on Proportion of females in the labour force / Proportion of males in the labour force variable. These two variables can be considered as almost independent.



### 2.1  Principal component analysis (PCA)

Principal Component Analysis (PCA) can be performed by two sightly different matrix decomposition methods from linear algebra: the Eigenvalue Decomposition and the Singular Value Decomposition (SVD).

There are two functions in the default package distribution of R that can be used to perform PCA: princomp() and prcomp(). The prcomp() function uses the SVD and is the preferred, more numerically accurate method.

Both methods quite literally decompose a data matrix into a product of smaller matrices, which let's us extract the underlying principal components. This makes it possible to approximate a lower dimensional representation of the data by choosing only a few principal components.

```{r}


# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)

# print out summaries of the non-standardized variables
summary(human)

```


### 2.2 PCA with a biplot.


A biplot is a way of visualizing the connections between two representations of the same data. First, a simple scatter plot is drawn where the observations are represented by two principal components (PC's). Then, arrows are drawn to visualize the connections between the original variables and the PC's. 

The following connections hold:

The angle between the arrows can be interpret as the correlation between the variables.
The angle between a variable and a PC axis can be interpret as the correlation between the two.
The length of the arrows are proportional to the standard deviations of the variables.


```{r}

# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```


### 2.3 Interpretation of results

- In bi-plot, the length of the arrow is proportional to standard deviation of features.
- Small angle implies high positive correlation.
- It can be observed that **GNI** has the largest length arrow indicating highest standard deviation.
- Angle between first principal component and **GNI** variable is almost zero which indicates a high positive correlation.
- Unfortunately, these kinds of results is due to non-standardized data.

### 3.1  Standardization


```{r}

# standardize the variables
human_std <- scale(human)

# print out summaries of the standardized variables
summary(human_std)

# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)

```


### 3.2 PCA with a biplot.

```{r}

# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```

### 3.3 Interpretation of results

 - We can Immediately notice that results with and without standardizing are different.
 
 - After standardization, each column variables have been scaled such that mean of the column data is zero and standard deviation is 1.
 
 - In bi-plot, the length of the arrow is proportional to standard deviation of features/variables.
 
 - Since data is standardized, we can observe almost similar lengths in all arrows. It concludes that all variables have equal standard deviations.

 - In bi-plot, Small angle implies high positive correlation.

 - We can see that **Labo.FM** variable is almost orthogonal to **Edu.Exp** variable. We do not see a strong correlation between  **Edu.Exp** variable  and **Labo.FM**. That is because Expected years of schooling does not depend on Proportion of females in the labour force / Proportion of males in the labour force variable. These two variables can be considered as almost independent.

 - It can be observed that there is a small angle between **Mat.Mor** and **Ado.Birth**. They are pointing to the same direction. That is due to the fact that **Ado.Birth** has a strong positive correlation with **Mat.Mor**. Maternal mortality ratio increases with Adolescent birth rate.
 
 - On the other hand, **Mat.Mor** and **Ado.Birth** almost align with first principle component. It implies that they contribute to  that dimension. 
 
 
### 5.1 Load the tea dataset from the package Factominer and explore the dataset

The Factominer package contains functions dedicated to multivariate explanatory data analysis. It contains for example methods (Multiple) Correspondence analysis , Multiple Factor analysis as well as PCA.

In the next exercises we are going to use the tea dataset. The dataset contains the answers of a questionnaire on tea consumption.


```{r}


library(FactoMineR)

# load the data set from FactoMineR-package
data("tea") 

# the structure and the dimensions of the data and visualize
str(tea)
dim(tea)
summary(tea)

library(tidyr)
# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- dplyr::select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)

# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
```

- In the summary, we can see eigenvalues corresponding to total of  11 dimensions.

### 5.2 Multiple Correspondence Analysis

Multiple Correspondence Analysis (MCA) is a method to analyze qualitative data and it is an extension of Correspondence analysis (CA). MCA can be used to detect patterns or structure in the data as well as in dimension reduction.


```{r}

# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)

# summary of the model
summary(mca)

# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
```


### 5.3 Interpretation of results

- In the above MCA factor map,  the variables are drawn only for the first two dimensions. 

- The plot above helps to identify categories that are the most correlated with each dimension. The squared correlations between categories and the dimensions are used as coordinates.

- It can be seen that, the category variables **alone** and **lunch** are the most correlated with dimension 1. Similarly, the variables   **sugar** and **No.Sugar** are the most correlated with dimension 2.

- Moreover, it is more likely to have **unpackaged** tea   in a **tea shop** while **tea bag** tea in **chain store**. These two pairs can be observed together in the plot.


### 5.4 MCA with a screeplot.


```{r}
# draw a screeplot of the MCA 

library("factoextra")
fviz_screeplot(mca, addlabels = TRUE, ylim = c(0, 45))

```

- We can use screeplot to visualize the percentages of inertia explained by each MCA dimensions.
- It can be seen that highest inertia is with dim 1 and dim-10 has the lowest inertia.

### 5.5 MCA with a biplot of both individuals and categories.

```{r}
# draw a biplot of the MCA and the original variables

mca <- MCA(tea_time, graph = FALSE)

library("factoextra")
# eig.val <- get_eigenvalue(mca)
# fviz_screeplot(mca, addlabels = TRUE, ylim = c(0, 45))
fviz_mca_biplot(mca, 
               repel = TRUE, # Avoid text overlapping (slow if many point)
               ggtheme = theme_minimal())

```

- The plot above helps to identify variables that are the most correlated with each dimension. The squared correlations between variables and the dimensions are used as coordinates.

- Active individuals are in blue in the plot.

- Active variable categories are in red in the plot.

- Individuals such as 171,277 and 278 are strongly correlated with dim 1 where as 72,270 and 15 are strongly correlated with dim 2.

<!-- ### 5.6 Correlation between categories and principal dimensions. -->


<!-- We are going to  visualize the correlation between variables and MCA principal dimensions. -->

<!-- ```{r} -->

<!-- library("factoextra") -->
<!-- fviz_mca_var(mca, choice = "mca", -->
<!--              repel = TRUE) -->

<!-- ``` -->

<!-- - The plot above helps to identify variables that are the most correlated with each dimension. The squared correlations between variables and the dimensions are used as coordinates. -->

<!-- - It can be seen that, the variable **sugar** is the most correlated with dimension 1. Similarly, the variable **lunch** is the most correlated with dimension 2. -->




Reference:

Datacamp : https://campus.datacamp.com/courses/helsinki-open-data-science/dimensionality-reduction-techniques

